{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toNVfMF2jVuY"
   },
   "source": [
    "# WrexGPT - example of usage\n",
    "- it was trained on google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XRGRn_GhNYo"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49mvE7YPhVMi",
    "outputId": "4386976b-e09f-4d8d-be4e-17f437e8dfdb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tiktoken\n",
    "!pip install ftfy\n",
    "import ftfy\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTQrwAKh6e8N"
   },
   "source": [
    "# ------ DEFINITIONS -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEQtLIyahLm5"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlQrZQwkht3o"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZwpHFDfjO5f"
   },
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, context_length, num_heads=12, dropout=0.1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "        if (dim_out % num_heads) != 0:\n",
    "            raise ValueError(\"dim_out must be divisible by num_heads\")\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.context_length = context_length\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_dim = dim_out // num_heads\n",
    "\n",
    "        self.w_query = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "        self.w_key = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "        self.w_value = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        self.out_projection = nn.Linear(dim_out, dim_out)\n",
    "        self.out_projection.RESIDUAL_INIT = 1\n",
    "\n",
    "    # INPUT x = (batch_size = 2, context_len = 3, dim_in = 968)\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        batch_size, context_length, dim_in = x.shape\n",
    "\n",
    "        # x = (batch_size = 2, context_len = 3, dim_out = 968)\n",
    "        q = self.w_query(x)\n",
    "        k = self.w_key(x)\n",
    "        v = self.w_value(x)\n",
    "\n",
    "        # we have to split each encoded sentence for head\n",
    "        # x = (batch_size = 2, context_len = 3, num_heads = 8, head_dim = 121)\n",
    "        q = q.view(batch_size,context_length, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size,context_length, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size,context_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        # now we have per batch-head sample, each batch, have heads wich have specific subspace of mapping\n",
    "        # x = (batch_size = 2, num_heads = 8,context_len = 3, head_dim = 121)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # now we want to have (batch_size, num_heads, context_len, context_len)\n",
    "        # x = (batch_size = 2, num_heads = 8, context_len = 3, context_len = 3)\n",
    "        att_score = q @ k.transpose(2, 3)\n",
    "        att_score = att_score / math.sqrt(self.head_dim)\n",
    "\n",
    "        # now masking\n",
    "        # but if sentences are not == context_len then mask is truncated\n",
    "        mask = self.mask.bool()[0:context_length, 0:context_length]\n",
    "        # trio mask\n",
    "        att_score = att_score.masked_fill_(mask, torch.finfo(att_score.dtype).min)\n",
    "        # padding mask\n",
    "        if padding_mask is not None:\n",
    "            att_score = att_score.masked_fill_(padding_mask, torch.finfo(att_score.dtype).min)\n",
    "\n",
    "\n",
    "\n",
    "        # softmax + dropout\n",
    "        att_score = self.softmax(att_score)\n",
    "        att_score = self.dropout(att_score)\n",
    "\n",
    "        # (batch_size, num_heads, context_len, context_len) * (batch_size, num_heads,context_len, head_dim)\n",
    "        # we want to have once again\n",
    "        # (batch_size = 2, num_heads = 8,context_len = 3, head_dim = 121) not attention scores\n",
    "        context_vec = att_score @ v\n",
    "\n",
    "        # now lets revert to\n",
    "        # (batch_size = 2,context_len = 3, num_heads = 8, head_dim = 121)\n",
    "        context_vec = context_vec.transpose(1, 2)\n",
    "\n",
    "        # now lets concatenate all the head so we have like at the beggining\n",
    "        # (batch_size = 2, context_len = 3, dim_out = 968)\n",
    "        context_vec = context_vec.contiguous().view(batch_size, context_length, self.dim_out)\n",
    "        context_vec = self.out_projection(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh7C5vEahvzu"
   },
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPg43ubqhw9F"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, dim_in, dim_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(dim_in, dim_hidden)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.l2 = nn.Linear(dim_hidden, dim_in)\n",
    "        self.l2.RESIDUAL_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4ieQ_Swhyeh"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iPNF7kgh1PU"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, dim_embedded):\n",
    "        super().__init__()\n",
    "        self.position_embedding = nn.Embedding(context_length, dim_embedded)\n",
    "\n",
    "    def forward(self, sentence_length, device):\n",
    "        # pos = (, context_len)\n",
    "        pos = torch.arange(sentence_length, device=device, dtype=torch.long)\n",
    "        # pos_emb = (, context_len, embedded_dim)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        return pos_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQnZ82IMh4Wz"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNZm3Kndh3L8"
   },
   "outputs": [],
   "source": [
    "# Decoder only, transormer like block\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, dim_embedded, context_length, num_heads, dropout=0.1, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.dim_embedded = dim_embedded\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.attention = MaskedMultiHeadAttention(dim_in=self.dim_embedded, dim_out=self.dim_embedded, context_length=context_length, num_heads=num_heads, dropout=dropout, bias=qkv_bias)\n",
    "\n",
    "        self.mlp = MultiLayerPerceptron(dim_in=self.dim_embedded, dim_hidden=self.dim_embedded*4)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(self.dim_embedded)\n",
    "        self.ln2 = nn.LayerNorm(self.dim_embedded)\n",
    "\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_mlp = nn.Dropout(dropout)\n",
    "\n",
    "    # x = (batch_size, context_len, embedded_dim)\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        # x = (batch_size, context_len, embedded_dim)\n",
    "        # throguh out all transformer it is the same size\n",
    "        identity = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.attention(x, mask)\n",
    "        x = self.dropout_attn(x)\n",
    "        x = x + identity\n",
    "\n",
    "        identity = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.dropout_mlp(x)\n",
    "        x = x + identity\n",
    "\n",
    "        # x = x + self.dropout_attn(self.attention(self.ln1(x)))\n",
    "        # x = x + self.dropout_mlp(self.mlp(self.ln2(x)))\n",
    "\n",
    "        # x = (batch_size, context_len, embedded_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRWmyfm-iRaK"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKwGGOk-iXwk"
   },
   "source": [
    "## Model hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yhp-710LiY1v"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Base model configuration that can be initialized for different GPT-2 variants.\"\"\"\n",
    "    dim_embedded: int\n",
    "    context_length: int\n",
    "    num_heads: int\n",
    "    layers: int\n",
    "    dropout: float\n",
    "    gradient_checkpointing: bool = True\n",
    "    vocab_size: int = 50258\n",
    "    padding_token: int = 50257\n",
    "\n",
    "    @classmethod\n",
    "    def from_preset(cls, preset: Literal[\"gpt2\", \"gpt2-mini\"] = \"gpt2\"):\n",
    "        \"\"\"Create a model configuration from a preset.\n",
    "\n",
    "        Args:\n",
    "            preset: Either \"gpt2\" or \"gpt2-mini\"\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            \"gpt2-test\": {\n",
    "                \"dim_embedded\": 2,\n",
    "                \"context_length\": 2,\n",
    "                \"num_heads\": 1,\n",
    "                \"layers\": 1,\n",
    "                \"dropout\": 0.2,\n",
    "            },\n",
    "            \"gpt2-mini\": {\n",
    "                \"dim_embedded\": 384,\n",
    "                \"context_length\": 256,\n",
    "                \"num_heads\": 6,\n",
    "                \"layers\": 6,\n",
    "                \"dropout\": 0.2,\n",
    "            },\n",
    "            \"gpt2\": {\n",
    "                \"dim_embedded\": 768,\n",
    "                \"context_length\": 1024,\n",
    "                \"num_heads\": 12,\n",
    "                \"layers\": 12,\n",
    "                \"dropout\": 0.1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if preset not in configs:\n",
    "            raise ValueError(f\"Unknown preset: {preset}. Choose from {list(configs.keys())}\")\n",
    "\n",
    "        return cls(**configs[preset])\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Pretty print configuration.\"\"\"\n",
    "        class_name = self.__class__.__name__\n",
    "        fields = []\n",
    "        max_key_length = max(len(key) for key in self.__dataclass_fields__.keys())\n",
    "\n",
    "        for key, value in self.__dict__.items():\n",
    "            padding = \" \" * (max_key_length - len(key))\n",
    "            fields.append(f\"  {key}{padding} = {value}\")\n",
    "\n",
    "        fields_str = \"\\n\".join(fields)\n",
    "        return f\"{class_name}(\\n{fields_str}\\n)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36ZzVEYGiVb0"
   },
   "source": [
    "## Training hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBBbR9X7iToi"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"Base training configuration that can be initialized for different GPT-2 variants.\"\"\"\n",
    "    total_steps: int\n",
    "    max_lr: float\n",
    "    batch_size: int\n",
    "    accumulation_batch_size: int\n",
    "    weight_decay: float\n",
    "    grad_clip: float\n",
    "    device: str\n",
    "    scale_factor: float\n",
    "    info_decay: int\n",
    "    checkpoint_decay: int\n",
    "    warmup_steps: int\n",
    "    early_stopper_patience: int\n",
    "    epochs: int = -1\n",
    "    padding_token: int = 50257\n",
    "    use_amp: bool = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set use_amp based on CUDA availability if not explicitly set.\"\"\"\n",
    "        if self.use_amp is None:\n",
    "            self.use_amp = torch.cuda.is_available()\n",
    "\n",
    "    @classmethod\n",
    "    def from_preset(cls, preset: Literal[\"gpt2\", \"gpt2-mini\"] = \"gpt2\"):\n",
    "        \"\"\"Create a training configuration from a preset.\n",
    "\n",
    "        Args:\n",
    "            preset: Either \"gpt2\" or \"gpt2-mini\"\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            \"gpt2-test\": {\n",
    "                \"total_steps\": 100,\n",
    "                \"max_lr\": 5e-4,\n",
    "                \"batch_size\": 256,\n",
    "                \"accumulation_batch_size\": 1024,\n",
    "                \"weight_decay\": 0.1,\n",
    "                \"grad_clip\": 1.0,\n",
    "                \"scale_factor\": 2.0,\n",
    "                \"warmup_steps\": 10,\n",
    "                \"early_stopper_patience\": 10,\n",
    "                \"info_decay\": 10,\n",
    "                \"checkpoint_decay\": 10\n",
    "            },\n",
    "            \"gpt2-mini\": {\n",
    "                \"total_steps\": 100000,\n",
    "                \"max_lr\": 2.5e-4,\n",
    "                \"batch_size\": 16,\n",
    "                \"accumulation_batch_size\": 64,\n",
    "                \"weight_decay\": 0.2,\n",
    "                \"grad_clip\": 1.0,\n",
    "                \"scale_factor\": 2.0,\n",
    "                \"warmup_steps\": 4000,\n",
    "                \"early_stopper_patience\": 18,\n",
    "                \"info_decay\": 500,\n",
    "                \"checkpoint_decay\": 500\n",
    "\n",
    "            },\n",
    "            \"gpt2\": {\n",
    "                \"total_steps\": 2000,\n",
    "                \"max_lr\": 6e-4,\n",
    "                \"batch_size\": 8,\n",
    "                \"accumulation_batch_size\": 64,\n",
    "                \"weight_decay\": 0.1,\n",
    "                \"grad_clip\": 1.0,\n",
    "                \"scale_factor\": 2.0,\n",
    "                \"warmup_steps\": 200,\n",
    "                \"early_stopper_patience\": 10,\n",
    "                \"info_decay\": 50,\n",
    "                \"checkpoint_decay\": 25\n",
    "            }\n",
    "        }\n",
    "\n",
    "        if preset not in configs:\n",
    "            raise ValueError(f\"Unknown preset: {preset}. Choose from {list(configs.keys())}\")\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        return cls(\n",
    "            **configs[preset],\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Pretty print configuration.\"\"\"\n",
    "        class_name = self.__class__.__name__\n",
    "        fields = []\n",
    "        max_key_length = max(len(key) for key in self.__dataclass_fields__.keys())\n",
    "\n",
    "        for key, value in self.__dict__.items():\n",
    "            padding = \" \" * (max_key_length - len(key))\n",
    "            fields.append(f\"  {key}{padding} = {value}\")\n",
    "\n",
    "        fields_str = \"\\n\".join(fields)\n",
    "        return f\"{class_name}(\\n{fields_str}\\n)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNfGl0qQiBJm"
   },
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3KLUPSkiDPj"
   },
   "outputs": [],
   "source": [
    "class WrexGPT(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.gradient_checkpointing = config.gradient_checkpointing\n",
    "\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.dim_embedded)\n",
    "        self.positional_encoding = PositionalEncoding(config.context_length, config.dim_embedded)\n",
    "\n",
    "        self.transformers = nn.ModuleList()\n",
    "        for _ in range(config.layers):\n",
    "            self.transformers.append(\n",
    "                TransformerDecoder(\n",
    "                    dim_embedded=config.dim_embedded,\n",
    "                    context_length=config.context_length,\n",
    "                    num_heads=config.num_heads,\n",
    "                    dropout=config.dropout,\n",
    "                    qkv_bias=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_ln = nn.LayerNorm(config.dim_embedded)\n",
    "        self.out_projection = nn.Linear(config.dim_embedded, config.vocab_size)\n",
    "\n",
    "        self.apply(self.__init_weights)\n",
    "\n",
    "    def __init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "\n",
    "            if hasattr(module, 'RESIDUAL_INIT'):\n",
    "                std /= (2*self.config.layers)**0.5\n",
    "\n",
    "            nn.init.normal_(module.weight, mean=0,std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "\n",
    "    # x = (batch_size, >= context_len)\n",
    "    def forward(self, x):\n",
    "        batch_size, sentence_length = x.shape\n",
    "        assert sentence_length <= self.config.context_length\n",
    "\n",
    "        # before all compute we have to calculate padding mask\n",
    "        padding_token = self.config.padding_token\n",
    "\n",
    "        # keep = (batch_size, context_len)\n",
    "        keep = (x != padding_token)  # (B, C) bool, True = token\n",
    "        # keep2d = (batch_size, context_len, context_en)\n",
    "        keep2d = keep.unsqueeze(2) & keep.unsqueeze(1)\n",
    "        mask_pad = ~keep2d\n",
    "        # mask_pad = (batch_size, 1, context_len, context_len)\n",
    "        mask_pad = mask_pad.unsqueeze(1)\n",
    "\n",
    "        # first we embed\n",
    "        # x = (batch_size, context_len, embedded_dim)\n",
    "        emb_x = self.embedding(x)\n",
    "\n",
    "        # then add postional learned encodding\n",
    "        # x = (batch_size, context_len, embedded_dim)\n",
    "        pos_x = self.positional_encoding(sentence_length, x.device)\n",
    "\n",
    "        # summing up\n",
    "        x = emb_x + pos_x\n",
    "\n",
    "        # now grind through transformers\n",
    "        for transformer in self.transformers:\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                x = torch.utils.checkpoint.checkpoint(transformer, x, mask_pad, use_reentrant=False)\n",
    "            else:\n",
    "                x = transformer(x, mask_pad)\n",
    "        # finally normalization and linear layer\n",
    "        # x = (batch_size, context_len, vocab_size)\n",
    "        x = self.out_ln(x)\n",
    "        x = self.out_projection(x)\n",
    "\n",
    "        # logits out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVn4MKOejCZL"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UgOUFkQmtYv"
   },
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKiZ1eF_jIzd"
   },
   "outputs": [],
   "source": [
    "class GPT2Trainer:\n",
    "    def __init__(self, model: nn.Module, config: TrainConfig, train_loader, val_loader=None, earlystopper=None, checkpoint_path=\"./checkpoint.pt\"):\n",
    "        self.model = model\n",
    "        self.model.to(config.device)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.earlystopper = earlystopper\n",
    "        self.is_earlystopped = False\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.config.padding_token)\n",
    "\n",
    "        if self.config.device == \"cuda\":\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.max_lr, fused=True)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.max_lr, fused=False)\n",
    "\n",
    "        self.scaler = torch.amp.GradScaler(device=config.device, enabled=(config.use_amp and \"cuda\" in config.device))\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.path = checkpoint_path\n",
    "\n",
    "        self.history = {\n",
    "            \"test_loss\": [],\n",
    "            \"test_acc\": [],\n",
    "            \"test_ppl\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"val_ppl\": [],\n",
    "            \"lr\": []\n",
    "        }\n",
    "\n",
    "\n",
    "        if (self.config.accumulation_batch_size % self.config.batch_size != 0) or (self.config.accumulation_batch_size < self.config.batch_size):\n",
    "            raise ValueError(\"Accumulated batch size has to be divisible by batch size and accumulated batch size cannot be less than batch size.\")\n",
    "        self.accumulation_step = self.config.accumulation_batch_size // self.config.batch_size\n",
    "\n",
    "        if (self.config.epochs == -1 and self.config.total_steps == -1):\n",
    "            raise ValueError(\"Bruh give number of epochs or total steps\")\n",
    "        elif (self.config.epochs != -1 and self.config.total_steps != -1):\n",
    "            raise ValueError(\"Please specify only one epoch or total steps\")\n",
    "\n",
    "        warmup_steps = 0\n",
    "        cosine_steps = 0\n",
    "\n",
    "        if self.config.epochs != -1:\n",
    "            self.epochs = self.config.epochs\n",
    "\n",
    "            # calculating all steps based on dataloader len and accumulation step size\n",
    "            self.total_steps = math.ceil((len(self.train_loader)/self.accumulation_step)*self.epochs)\n",
    "            warmup_steps = self.config.warmup_steps\n",
    "            cosine_steps = self.total_steps - self.config.warmup_steps\n",
    "        elif self.config.total_steps != -1:\n",
    "            self.total_steps = self.config.total_steps\n",
    "            self.epochs = self.config.epochs\n",
    "            warmup_steps = self.config.warmup_steps\n",
    "            cosine_steps = self.total_steps - self.config.warmup_steps\n",
    "\n",
    "\n",
    "\n",
    "        warmup = LinearLR(self.optimizer,\n",
    "                  start_factor=1e-8,\n",
    "                  end_factor=1.0,\n",
    "                  total_iters=max(0,warmup_steps))\n",
    "\n",
    "        cosine = CosineAnnealingLR(self.optimizer,\n",
    "                                   T_max= max(0,cosine_steps),\n",
    "                                   eta_min=0)\n",
    "\n",
    "        self.scheduler = SequentialLR(\n",
    "            self.optimizer,\n",
    "            schedulers=[warmup, cosine],\n",
    "            milestones=[self.config.warmup_steps]\n",
    "        )\n",
    "\n",
    "    def __load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.path, map_location=self.config.device)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "        self.config.__dict__.update(checkpoint[\"config\"])\n",
    "\n",
    "        if self.scheduler and checkpoint[\"scheduler_state_dict\"] is not None:\n",
    "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "        if self.config.use_amp and checkpoint[\"scaler_state_dict\"] is not None:\n",
    "            self.scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "\n",
    "        if checkpoint[\"earlystopper_state_dict\"] is not None and self.earlystopper is not None:\n",
    "            self.earlystopper.load_state_dict(checkpoint[\"earlystopper_state_dict\"])\n",
    "\n",
    "        self.is_earlystopped = checkpoint[\"is_earlystopped\"]\n",
    "\n",
    "        self.history = checkpoint[\"history\"]\n",
    "\n",
    "        self.path = checkpoint[\"path\"]\n",
    "        self.accumulation_step = checkpoint[\"accumulation_step\"]\n",
    "\n",
    "        self.current_step = checkpoint[\"current_step\"]\n",
    "        self.current_epoch = checkpoint[\"current_epoch\"]\n",
    "\n",
    "        print(f\"Checkpoint loaded from {self.path}\")\n",
    "\n",
    "    def __save_checkpoint(self):\n",
    "\n",
    "        checkpoint = {\n",
    "            \"current_epoch\": self.current_epoch,\n",
    "            \"current_step\": self.current_step,\n",
    "            \"accumulation_step\": self.accumulation_step,\n",
    "            \"path\": self.path,\n",
    "            \"model_state_dict\": self.model.state_dict(),\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": self.scheduler.state_dict() if self.scheduler else None,\n",
    "            \"earlystopper_state_dict\": self.earlystopper.state_dict() if self.earlystopper else None,\n",
    "            \"is_earlystopped\": self.is_earlystopped,\n",
    "            \"scaler_state_dict\": self.scaler.state_dict() if self.config.use_amp else None,\n",
    "            \"history\": self.history,\n",
    "            \"config\": self.config.__dict__\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, self.path)\n",
    "\n",
    "    def __calculate_loss(self, logits, y):\n",
    "        # logits = (B, T, V)\n",
    "        # y      = (B, T)\n",
    "        B, T, V = logits.shape\n",
    "        logits = logits.view(B * T, V)\n",
    "        y = y.view(B * T)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __calculate_accuracy(self, logits, y):\n",
    "        # logits = (B, T, V)\n",
    "        # y      = (B, T)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)  # (B, T)\n",
    "        mask = (y != self.config.padding_token)\n",
    "\n",
    "        if mask.sum() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        correct = ((preds == y) & mask).float().sum()\n",
    "        total = mask.float().sum()\n",
    "        return (correct / total).item()\n",
    "\n",
    "    def __train_epoch(self):\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        local_accumulation_step = min(self.accumulation_step, len(self.train_loader))\n",
    "\n",
    "        for i,(X, y) in enumerate(tqdm(self.train_loader, desc=f\"Training {self.current_epoch+1}\", leave=False)):\n",
    "            X,y = X.to(self.config.device, non_blocking=True), y.to(self.config.device, non_blocking=True)\n",
    "            if self.config.use_amp:\n",
    "                with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                    logits = self.model(X)\n",
    "                    raw_loss = self.__calculate_loss(logits, y)\n",
    "                    loss = raw_loss / local_accumulation_step\n",
    "\n",
    "                self.scaler.scale(loss).backward()\n",
    "\n",
    "                # doing gradient step only after \"accumulation_step\"\n",
    "                if (i+1)%self.accumulation_step == 0 or (i+1) >= len(self.train_loader):\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.config.grad_clip)\n",
    "\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                    self.scaler.update()\n",
    "\n",
    "                    # gradient reset\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                    self.current_step += 1\n",
    "                    local_accumulation_step = min(self.accumulation_step, len(self.train_loader) - (i + 1))\n",
    "\n",
    "            else:\n",
    "                logits = self.model(X)\n",
    "                raw_loss = self.__calculate_loss(logits, y)\n",
    "                loss = raw_loss / local_accumulation_step\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                # doing gradient step only after \"accumulation_step\"\n",
    "                if (i+1)%self.accumulation_step == 0 or (i+1) >= len(self.train_loader):\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler is not None:\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                    # gradient reset\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                    self.current_step += 1\n",
    "                    local_accumulation_step = min(self.accumulation_step, len(self.train_loader) - (i + 1))\n",
    "\n",
    "            # LOSS AND ACC CALCULATION\n",
    "            running_loss += raw_loss.item()\n",
    "            running_acc += self.__calculate_accuracy(logits.detach(), y)\n",
    "            total_batches += 1\n",
    "\n",
    "\n",
    "\n",
    "        # STATISTICS\n",
    "        avg_loss = running_loss / max(1,total_batches)\n",
    "        avg_acc = running_acc / max(1,total_batches)\n",
    "        ppl = math.exp(min(avg_loss, 20))\n",
    "\n",
    "        return avg_loss, avg_acc, ppl\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __validate_model(self):\n",
    "        if self.val_loader is None:\n",
    "            return 0.0, 0.0, 0.0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in self.val_loader:\n",
    "            x = x.to(self.config.device, non_blocking=True)\n",
    "            y = y.to(self.config.device, non_blocking=True)\n",
    "\n",
    "            logits = self.model(x)\n",
    "            loss = self.__calculate_loss(logits, y)\n",
    "            acc = self.__calculate_accuracy(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / max(1, n_batches)\n",
    "        avg_acc = total_acc / max(1, n_batches)\n",
    "        ppl = math.exp(min(avg_loss, 20))\n",
    "\n",
    "        return avg_loss, avg_acc, ppl\n",
    "\n",
    "    def train_epochs(self, revive_mode=False):\n",
    "        if self.config.epochs == -1:\n",
    "            raise ValueError(\"Number of epochs cannot be -1. Please speicfy nubmer of epochs.\")\n",
    "\n",
    "        if revive_mode:\n",
    "            self.__load_checkpoint()\n",
    "\n",
    "        # if we were early stopped we do not go with training\n",
    "        if self.is_earlystopped:\n",
    "            return self.history\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Total epochs: {self.epochs}\")\n",
    "        print(f\"Current epoch: {self.current_epoch}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for epoch in tqdm(range(self.current_epoch, self.epochs), desc=f\"Epoch {self.current_epoch+1}/{self.epochs}\"):\n",
    "\n",
    "            train_loss, train_acc, train_ppl = self.__train_epoch()\n",
    "            print(\"\\nValidating... \", end='')\n",
    "            val_loss, val_acc, val_ppl = self.__validate_model()\n",
    "\n",
    "            self.history[\"test_loss\"].append(train_loss)\n",
    "            self.history[\"test_acc\"].append(train_acc)\n",
    "            self.history[\"test_ppl\"].append(train_ppl)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_acc\"].append(val_acc)\n",
    "            self.history[\"val_ppl\"].append(val_ppl)\n",
    "            self.history[\"lr\"].append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # ======== PRINT RESULTS ========\n",
    "            print(f\"\\nEpoch [{epoch + 1}/{self.epochs}]\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train PPL: {train_ppl:.2f}\")\n",
    "            print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f} | Val   PPL: {val_ppl:.2f}\")\n",
    "            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6e}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            # ======== EARLY STOPPING ========\n",
    "            if self.earlystopper is not None:\n",
    "                should_stop = self.earlystopper.step(val_loss, self.model, epoch)\n",
    "\n",
    "                if should_stop:\n",
    "                    self.is_earlystopped = True\n",
    "                    print(\"\\nCheckpoint and early stopping... \")\n",
    "                    break\n",
    "\n",
    "            # ======== UPDATE EPOCHS ========\n",
    "            self.current_epoch += 1\n",
    "\n",
    "            # ======== SAVE CHECKPOINT ========\n",
    "            self.__save_checkpoint()\n",
    "\n",
    "\n",
    "        print(\"\\nTraining finished.\")\n",
    "        return self.history\n",
    "\n",
    "    def train_steps(self, revive_mode=False):\n",
    "        if self.config.total_steps == -1:\n",
    "            raise ValueError(\"total_steps cannot be -1. Specify the number of steps\")\n",
    "\n",
    "        if revive_mode:\n",
    "            self.__load_checkpoint()\n",
    "\n",
    "        # if we were early stopped we do not go with training\n",
    "        if self.is_earlystopped:\n",
    "            return self.history\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Total steps: {self.total_steps}\")\n",
    "        print(f\"Current step: {self.current_step}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        progress_bar = tqdm(total=self.total_steps)\n",
    "        progress_bar.update(self.current_step)\n",
    "        progress_bar.refresh()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if self.current_step >= self.total_steps:\n",
    "            working = False\n",
    "        else:\n",
    "            working = True\n",
    "\n",
    "        while working:\n",
    "            local_accumulation_step = min(self.accumulation_step, len(self.train_loader))\n",
    "            for i, (X, y) in enumerate(self.train_loader):\n",
    "                X, y = X.to(self.config.device, non_blocking=True), y.to(self.config.device, non_blocking=True)\n",
    "\n",
    "                # COMPUTING AND STEPPING\n",
    "                if self.config.use_amp:\n",
    "                    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                        logits = self.model(X)\n",
    "                        raw_loss = self.__calculate_loss(logits, y)\n",
    "                        loss = raw_loss / local_accumulation_step\n",
    "\n",
    "                    self.scaler.scale(loss).backward()\n",
    "\n",
    "                    # doing gradient step only after \"accumulation_step\"\n",
    "                    if (i + 1) % self.accumulation_step == 0 or (i + 1) >= len(self.train_loader):\n",
    "                        self.scaler.unscale_(self.optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
    "                                                       max_norm=self.config.grad_clip)\n",
    "\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        if self.scheduler is not None:\n",
    "                            self.scheduler.step()\n",
    "\n",
    "                        self.scaler.update()\n",
    "\n",
    "                        # gradient reset\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                        self.current_step += 1\n",
    "                        progress_bar.update(1)\n",
    "                        local_accumulation_step = min(self.accumulation_step, len(self.train_loader) - (i+1))\n",
    "                else:\n",
    "                    logits = self.model(X)\n",
    "                    raw_loss = self.__calculate_loss(logits, y)\n",
    "                    loss = raw_loss / local_accumulation_step\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    # doing gradient step only after \"accumulation_step\"\n",
    "                    if (i + 1) % self.accumulation_step == 0 or (i + 1) >= len(self.train_loader):\n",
    "\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
    "\n",
    "                        self.optimizer.step()\n",
    "                        if self.scheduler is not None:\n",
    "                            self.scheduler.step()\n",
    "\n",
    "                        # gradient reset\n",
    "                        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                        self.current_step += 1\n",
    "                        progress_bar.update(1)\n",
    "                        local_accumulation_step = min(self.accumulation_step, len(self.train_loader) - (i+1))\n",
    "\n",
    "\n",
    "\n",
    "                # LOSS AND ACC CALCULATION\n",
    "                running_loss += raw_loss.item()\n",
    "                running_acc += self.__calculate_accuracy(logits.detach(), y)\n",
    "                total_batches += 1\n",
    "\n",
    "                # INFO CHECKPOINT\n",
    "                if ((self.current_step % self.config.info_decay == 0) and ((i + 1) % self.accumulation_step == 0 or (i + 1) >= len(self.train_loader))) or (self.current_step == 0 and i == 0):\n",
    "                    # STATISTICS\n",
    "                    print(\"\\nValidating... \", end='')\n",
    "\n",
    "                    train_loss = running_loss / max(1, total_batches)\n",
    "                    train_acc = running_acc / max(1, total_batches)\n",
    "                    train_ppl = math.exp(min(train_loss, 20))\n",
    "\n",
    "                    val_loss, val_acc, val_ppl = self.__validate_model()\n",
    "\n",
    "                    # saving hisotry\n",
    "                    self.history[\"test_loss\"].append(train_loss)\n",
    "                    self.history[\"test_acc\"].append(train_acc)\n",
    "                    self.history[\"test_ppl\"].append(train_ppl)\n",
    "                    self.history[\"val_loss\"].append(val_loss)\n",
    "                    self.history[\"val_acc\"].append(val_acc)\n",
    "                    self.history[\"val_ppl\"].append(val_ppl)\n",
    "                    self.history[\"lr\"].append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                    # reset stats\n",
    "                    running_loss = 0.0\n",
    "                    running_acc = 0.0\n",
    "                    total_batches = 0\n",
    "\n",
    "                    # write down\n",
    "                    print(\n",
    "                        f\"[Step {self.current_step}] \"\n",
    "                        f\"Train Loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | \"\n",
    "                        f\"Val Loss: {val_loss:.4f} | Val acc: {val_acc:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    # early stopper step\n",
    "                    if self.earlystopper is not None:\n",
    "                        if self.earlystopper.step(val_loss, self.model, self.current_step):\n",
    "                            self.is_earlystopped = True\n",
    "                            print(\"\\nCheckpoint and early stopping... \")\n",
    "                            self.__save_checkpoint()\n",
    "                            working = False\n",
    "                            break\n",
    "\n",
    "                    self.model.train()\n",
    "\n",
    "                # SAVING CHECKPOINT\n",
    "                if (self.current_step % self.config.checkpoint_decay == 0) and ((i + 1) % self.accumulation_step == 0 or (i + 1) >= len(self.train_loader)):\n",
    "                    print(\"\\nCheckpoint... \")\n",
    "                    self.__save_checkpoint()\n",
    "\n",
    "                # BREAKING OUT THE LOOP\n",
    "                if  self.current_step >= self.total_steps:\n",
    "                    working = False\n",
    "                    break\n",
    "\n",
    "        return self.history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij6Uh1PkmqEe"
   },
   "source": [
    "## Early Stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9biD74BmwDH"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=1e-4, path=\"best_model.pt\"):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "        self.best_model = None\n",
    "\n",
    "    def step(self, val_loss, model, epoch):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "    def state_dict(self):\n",
    "      return {\n",
    "          \"patience\": self.patience,\n",
    "          \"min_delta\": self.min_delta,\n",
    "          \"path\": self.path,\n",
    "          \"best_loss\": self.best_loss,\n",
    "          \"counter\": self.counter,\n",
    "          \"best_epoch\": self.best_epoch,\n",
    "      }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.patience = state_dict[\"patience\"]\n",
    "        self.min_delta = state_dict[\"min_delta\"]\n",
    "        self.path = state_dict[\"path\"]\n",
    "        self.best_loss = state_dict[\"best_loss\"]\n",
    "        self.counter = state_dict[\"counter\"]\n",
    "        self.best_epoch = state_dict[\"best_epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoRleeUcIqbG"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9k6zMq90IsdV"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, padding_token=50257):\n",
    "        super().__init__()\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "        gpt2_base_tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
    "\n",
    "        tokenizer = tiktoken.Encoding(\n",
    "            name=\"gpt2-extra-tokens\",\n",
    "            pat_str=gpt2_base_tokenizer._pat_str,\n",
    "            mergeable_ranks=gpt2_base_tokenizer._mergeable_ranks,\n",
    "            special_tokens={\n",
    "                **gpt2_base_tokenizer._special_tokens,\n",
    "                \"<|pad|>\": self.padding_token\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        cleared_text = ftfy.fix_text(text)\n",
    "        cleared_text = cleared_text.replace(\"\\r\\n\", \"\\n\")\n",
    "        tokens = self.tokenizer.encode(cleared_text)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list):\n",
    "        return self.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djyf3R4MXmDP"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZdPn_npXngc"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens_path, context_len, tokens = None):\n",
    "        if tokens is None:\n",
    "            self.tokens = np.load(tokens_path, mmap_mode=\"r\")\n",
    "        else:\n",
    "            self.tokens = tokens\n",
    "        self.context_len = context_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # cuz we have to left one word for last prediction\n",
    "        return len(self.tokens) - self.context_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return x, y\n",
    "        x = torch.tensor(self.tokens[idx:idx + self.context_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[idx+1:idx + self.context_len + 1], dtype=torch.long)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZlmvooyXpnj"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDatasetWithStride(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens_path, context_len, tokens = None, stride=1, padding_token=50257):\n",
    "        if tokens is None:\n",
    "            self.tokens = np.load(tokens_path, mmap_mode=\"r\")\n",
    "        else:\n",
    "            self.tokens = tokens\n",
    "        self.context_len = context_len\n",
    "        self.stride = int(stride)\n",
    "        self.padding_token = padding_token\n",
    "\n",
    "    def __len__(self):\n",
    "        # cuz we have to left one word for last prediction\n",
    "        return math.ceil(len(self.tokens) / self.stride)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return x, y\n",
    "        current_index = (idx*self.stride)\n",
    "        x = torch.tensor(self.tokens[current_index: current_index + self.context_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.tokens[current_index+1: current_index + self.context_len + 1], dtype=torch.long)\n",
    "\n",
    "        if x.shape[0] != self.context_len:\n",
    "            padding_needed = self.context_len - x.shape[0]\n",
    "            x = torch.nn.functional.pad(x, (0, padding_needed), value=self.padding_token)\n",
    "\n",
    "        if y.shape[0] != self.context_len:\n",
    "            padding_needed = self.context_len - y.shape[0]\n",
    "            y = torch.nn.functional.pad(y, (0, padding_needed), value=self.padding_token)\n",
    "\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkLLaXWMGRDT"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaheLsOHGS6J"
   },
   "outputs": [],
   "source": [
    "class AutoregressiveGenerator:  # Nie musi dziedziczy po nn.Module, jeli nie ma trenowalnych parametrw\n",
    "    def __init__(self, model, config, device):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.tokenizer = Tokenizer()  # Przekazujemy instancj, a nie tworzymy now\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, text: str, max_new_tokens: int = 50, temperature=1.0, top_k=None, greedy=True):\n",
    "        self.model.eval()\n",
    "        tokens = torch.tensor(self.tokenizer.encode(text), dtype=torch.long, device=self.device)\n",
    "        # added bacth\n",
    "        tokens = tokens.view(1,-1)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            cond_tokens = None\n",
    "            if tokens.shape[1] <= self.config.context_length:\n",
    "                cond_tokens = tokens\n",
    "            else:\n",
    "                cond_tokens = tokens[:,-self.config.context_length:]\n",
    "            logits = self.model(cond_tokens)\n",
    "            logits = logits[:, -1, :]\n",
    "            logits = logits / temperature\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if greedy:\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        return self.tokenizer.decode(tokens[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--e-hiQblEdw"
   },
   "source": [
    "# ------- TRAINING GPT2 --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfHyMnGMu-ql"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwbTiVTKjtr5"
   },
   "outputs": [],
   "source": [
    "gpt_config = ModelConfig.from_preset(\"gpt2\")\n",
    "train_config = TrainConfig.from_preset(\"gpt2\")\n",
    "\n",
    "drive_path = \"./drive/MyDrive/Models_state/wrexgpt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3y6z9OqwjOnb"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhVxIK_Jjb92"
   },
   "source": [
    "## Data load and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoWGf8w8jTYB",
    "outputId": "dbbf37be-941a-49d9-97b9-e453737c1730"
   },
   "outputs": [],
   "source": [
    "tokenizer_class = Tokenizer()\n",
    "\n",
    "input_path = drive_path + \"/dataset/input.txt\"\n",
    "out_path = drive_path + \"/dataset/input_tokens.npy\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = tokenizer_class.encode(text)\n",
    "\n",
    "arr = np.array(tokens, dtype=np.int32)\n",
    "np.save(out_path, arr)\n",
    "\n",
    "print(\"tokens:\", arr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6RfyzyqjYc-"
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pc_N0IUjXef",
    "outputId": "d0848bd3-5c07-43ee-e055-4f96f45d2c81"
   },
   "outputs": [],
   "source": [
    "tokens = np.load(drive_path + \"/dataset/input_tokens.npy\")\n",
    "train_data = tokens[:int(0.9 * len(tokens))]\n",
    "val_data = tokens[int(0.9 * len(tokens)):]\n",
    "\n",
    "train_ds = ShakespeareDatasetWithStride(\"\", context_len=gpt_config.context_length, tokens=train_data, stride=gpt_config.context_length/2, padding_token=gpt_config.padding_token)\n",
    "val_ds = ShakespeareDatasetWithStride(\"\", context_len=gpt_config.context_length, tokens=val_data, stride=gpt_config.context_length/4, padding_token=gpt_config.padding_token)\n",
    "\n",
    "# train_ds = ShakespeareDataset(\"\", context_len=gpt_config.context_length, tokens=train_data)\n",
    "# val_ds = ShakespeareDataset(\"\", context_len=gpt_config.context_length, tokens=train_data)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  train_loader = DataLoader(dataset=train_ds, batch_size=train_config.batch_size, shuffle=True, pin_memory=True)\n",
    "  val_loader = DataLoader(dataset=val_ds, batch_size=train_config.batch_size, shuffle=True, pin_memory=True)\n",
    "else:\n",
    "  train_loader = DataLoader(dataset=train_ds, batch_size=train_config.batch_size, shuffle=True)\n",
    "  val_loader = DataLoader(dataset=val_ds, batch_size=train_config.batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Train size: {train_loader.__len__()}\\nVal size: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7I6iyEAmNfK"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSXTnxswkho1",
    "outputId": "7dab7589-0f2f-489e-f3a8-75cb0e0f2947"
   },
   "outputs": [],
   "source": [
    "# updating number of\n",
    "\n",
    "model = WrexGPT(config = gpt_config)\n",
    "checkpoint_path = drive_path + \"/checkpoints/checkpoints.pt\"\n",
    "best_model_path = drive_path + \"/checkpoints/best_model.pt\"\n",
    "\n",
    "\n",
    "earlystopper = EarlyStopping(patience=train_config.early_stopper_patience, path=best_model_path)\n",
    "\n",
    "trainer = GPT2Trainer(model=model, config=train_config, train_loader=train_loader, val_loader=val_loader, checkpoint_path=checkpoint_path, earlystopper=earlystopper)\n",
    "\n",
    "print(gpt_config)\n",
    "print(train_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681,
     "referenced_widgets": [
      "9e51dfcdb4084008baafe01af2c83a4a",
      "f46e48d9661740a49b6d3ad48d06eb6f",
      "93100088dbed46eda33f167195e23d7f",
      "8d164cbb9c5444fba0e9d30ba7ec0b82",
      "f5fb22ca7af9434d963f90bd9df7b62b",
      "1a42979759f7487a9e4ec9d044fd8dda",
      "ce93a61412b746ed8329683f00f25d41",
      "d9666d60fc7640b3a7f4641a13587ab6",
      "530259697bc04a03ac06fffe0eb87e9f",
      "d0205937d961432bb724be3231185850",
      "40efb2a1885e4e2886a3d1d0580b4a13"
     ]
    },
    "id": "wrh1ajlw4dSa",
    "outputId": "172c74cb-420e-4310-950e-33c22fe6cf93"
   },
   "outputs": [],
   "source": [
    "history = trainer.train_steps(revive_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LznegWZJfJIu",
    "outputId": "75a31cf9-e6fb-42ac-c3fe-b3f08c7a13e3"
   },
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5YbUkgKUUcP"
   },
   "source": [
    "# Test and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "F06i_mV-S-cS",
    "outputId": "2d4ef3a5-c7e2-4b12-b097-91d7bd1ad2b0"
   },
   "outputs": [],
   "source": [
    "checkpoint_mode = True\n",
    "if checkpoint_mode:\n",
    "    history = torch.load(checkpoint_path, map_location=train_config.device)['history']\n",
    "\n",
    "epochs = range(1, len(history['test_loss']) + 1)\n",
    "\n",
    "# Tworzenie figury z 3 subpotami\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Wykres Loss\n",
    "axs[0].plot(epochs, history['test_loss'], 'b', label='Test Loss')\n",
    "axs[0].plot(epochs, history['val_loss'], 'r', label='Val Loss')\n",
    "axs[0].set_title('Model Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# 2. Wykres Accuracy\n",
    "axs[1].plot(epochs, history['test_acc'], 'b', label='Test Acc')\n",
    "axs[1].plot(epochs, history['val_acc'], 'r', label='Val Acc')\n",
    "axs[1].set_title('Model Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# 3. Wykres Perplexity (PPL)\n",
    "axs[2].plot(epochs, history['test_ppl'], 'b', label='Test PPL')\n",
    "axs[2].plot(epochs, history['val_ppl'], 'r', label='Val PPL')\n",
    "axs[2].set_title('Model Perplexity (PPL)')\n",
    "axs[2].set_xlabel('Epochs')\n",
    "axs[2].set_ylabel('PPL')\n",
    "axs[2].legend()\n",
    "axs[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrqVG_vmVOT6",
    "outputId": "85dd6367-7445-4941-a409-30fd1343095b"
   },
   "outputs": [],
   "source": [
    "model = WrexGPT(gpt_config)\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=train_config.device))\n",
    "\n",
    "generator = AutoregressiveGenerator(model,gpt_config, \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "text = \"Be or not to be\"\n",
    "\n",
    "genereted_text = generator.generate(text, max_new_tokens=30, greedy=False)\n",
    "\n",
    "print(genereted_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JEQtLIyahLm5",
    "oh7C5vEahvzu",
    "k4ieQ_Swhyeh",
    "JQnZ82IMh4Wz",
    "UKwGGOk-iXwk",
    "DNfGl0qQiBJm",
    "ij6Uh1PkmqEe",
    "3y6z9OqwjOnb",
    "V6RfyzyqjYc-"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a42979759f7487a9e4ec9d044fd8dda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40efb2a1885e4e2886a3d1d0580b4a13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "530259697bc04a03ac06fffe0eb87e9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d164cbb9c5444fba0e9d30ba7ec0b82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0205937d961432bb724be3231185850",
      "placeholder": "",
      "style": "IPY_MODEL_40efb2a1885e4e2886a3d1d0580b4a13",
      "value": "650/2000[2:33:45&lt;11:34:22,30.86s/it]"
     }
    },
    "93100088dbed46eda33f167195e23d7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9666d60fc7640b3a7f4641a13587ab6",
      "max": 2000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_530259697bc04a03ac06fffe0eb87e9f",
      "value": 650
     }
    },
    "9e51dfcdb4084008baafe01af2c83a4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f46e48d9661740a49b6d3ad48d06eb6f",
       "IPY_MODEL_93100088dbed46eda33f167195e23d7f",
       "IPY_MODEL_8d164cbb9c5444fba0e9d30ba7ec0b82"
      ],
      "layout": "IPY_MODEL_f5fb22ca7af9434d963f90bd9df7b62b"
     }
    },
    "ce93a61412b746ed8329683f00f25d41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0205937d961432bb724be3231185850": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9666d60fc7640b3a7f4641a13587ab6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46e48d9661740a49b6d3ad48d06eb6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a42979759f7487a9e4ec9d044fd8dda",
      "placeholder": "",
      "style": "IPY_MODEL_ce93a61412b746ed8329683f00f25d41",
      "value": "32%"
     }
    },
    "f5fb22ca7af9434d963f90bd9df7b62b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
