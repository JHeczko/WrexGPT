# WrexGPT
## Description
Implementation of GPT-2

## Project specification
### Libraries

### Architecture

### Dataset

### Training

## Articles
Based on:
- "Attention Is All You Need": [attention](https://arxiv.org/pdf/1706.03762)
- "Improving Language Understanding by Generative Pre-Training": [gpt-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- "Language Models are Unsupervised Multitask Learners": [gpt-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

